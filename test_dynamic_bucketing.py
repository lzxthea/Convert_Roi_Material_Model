# encoding: utf-8
"""
åŠ¨æ€åˆ†æ¡¶åŠŸèƒ½æµ‹è¯•è„šæœ¬
"""

import tensorflow as tf
import numpy as np
import os
import json
from dynamic_bucketing_simple import SimpleDynamicBucketing, create_dynamic_bucket_feats_simple

def test_dynamic_bucketing():
    """
    æµ‹è¯•åŠ¨æ€åˆ†æ¡¶åŠŸèƒ½
    """
    print("=== åŠ¨æ€åˆ†æ¡¶åŠŸèƒ½æµ‹è¯• ===")
    
    # 1. åˆå§‹åŒ–åŠ¨æ€åˆ†æ¡¶å™¨
    print("1. åˆå§‹åŒ–åŠ¨æ€åˆ†æ¡¶å™¨...")
    dynamic_bucketing = SimpleDynamicBucketing(
        num_batches_for_stats=100,  # ä½¿ç”¨100ä¸ªbatchè¿›è¡Œæµ‹è¯•
        quantiles=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
        min_bucket_size=5,
        max_bucket_size=20,
        save_path="./test_bucket_points.json"
    )
    
    # 2. æ¨¡æ‹Ÿç‰¹å¾æ•°æ®æ”¶é›†
    print("2. æ”¶é›†ç‰¹å¾ç»Ÿè®¡ä¿¡æ¯...")
    feature_names = ['fc_send_cnt_30m_qc_product_id_qp_roi2', 
                     'fc_show_cnt_30m_qc_product_id_qp_roi2',
                     'fc_click_cnt_30m_qc_product_id_qp_roi2']
    
    batch_size = 100
    
    for batch_idx in range(100):  # æ¨¡æ‹Ÿ100ä¸ªbatch
        # ç”Ÿæˆæ¨¡æ‹Ÿç‰¹å¾æ•°æ®
        features = {}
        for feat_name in feature_names:
            if 'cnt' in feat_name:
                # è®¡æ•°ç‰¹å¾ï¼šå¤§éƒ¨åˆ†ä¸º0ï¼Œå°‘æ•°ä¸ºè¾ƒå¤§å€¼
                values = np.random.exponential(scale=5.0, size=batch_size)
                values = np.where(np.random.random(batch_size) < 0.8, 0, values)
            else:
                # å…¶ä»–ç‰¹å¾ï¼šæ­£æ€åˆ†å¸ƒ
                values = np.random.normal(10, 5, size=batch_size)
                values = np.maximum(values, 0)
            
            features[feat_name] = tf.constant(values.reshape(-1, 1), dtype=tf.float32)
        
        # æ”¶é›†ç‰¹å¾ç»Ÿè®¡ä¿¡æ¯
        dynamic_bucketing.collect_feature_stats(features, feature_names)
        
        if batch_idx % 20 == 0:
            print(f"  å·²å¤„ç† {batch_idx} ä¸ªbatch")
    
    print("3. ç‰¹å¾ç»Ÿè®¡ä¿¡æ¯æ”¶é›†å®Œæˆ")
    
    # 3. æ£€æŸ¥ç”Ÿæˆçš„åˆ†æ¡¶ç‚¹
    print("4. æ£€æŸ¥ç”Ÿæˆçš„åˆ†æ¡¶ç‚¹...")
    all_bucket_points = dynamic_bucketing.get_all_bucket_points()
    
    for feat_name, bucket_points in all_bucket_points.items():
        print(f"  {feat_name}: {len(bucket_points)} ä¸ªåˆ†æ¡¶ç‚¹")
        print(f"    åˆ†æ¡¶ç‚¹: {bucket_points[:3]}...{bucket_points[-3:]}")
    
    # 4. æµ‹è¯•åˆ†æ¡¶ç‚¹ä¿å­˜å’ŒåŠ è½½
    print("5. æµ‹è¯•åˆ†æ¡¶ç‚¹ä¿å­˜å’ŒåŠ è½½...")
    
    # åˆ›å»ºæ–°çš„åˆ†æ¡¶å™¨å®ä¾‹
    new_bucketing = SimpleDynamicBucketing(
        num_batches_for_stats=100,
        save_path="./test_bucket_points.json"
    )
    
    # åŠ è½½åˆ†æ¡¶ç‚¹
    loaded_points = new_bucketing.load_bucket_points()
    print(f"  åŠ è½½äº† {len(loaded_points)} ä¸ªç‰¹å¾çš„åˆ†æ¡¶ç‚¹")
    
    # 5. æµ‹è¯•åŠ¨æ€åˆ†æ¡¶å¤„ç†
    print("6. æµ‹è¯•åŠ¨æ€åˆ†æ¡¶å¤„ç†...")
    
    # åŸå§‹é™æ€åˆ†æ¡¶é…ç½®
    dense_features_1d = {
        'fc_send_cnt_30m_qc_product_id_qp_roi2': [0.001, 7.0, 11.0, 19.0, 26.0, 34.0, 42.0, 51.0, 60.0, 74.0],
        'fc_show_cnt_30m_qc_product_id_qp_roi2': [0.001, 7.0, 11.0, 19.0, 26.0, 34.0, 42.0, 51.0, 60.0, 74.1],
        'fc_click_cnt_30m_qc_product_id_qp_roi2': [0.001, 7.0, 11.0, 19.0, 26.0, 34.0, 42.0, 51.0, 60.0, 74.2],
    }
    
    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    test_features = {}
    for feat_name in feature_names:
        if 'cnt' in feat_name:
            values = np.random.exponential(scale=5.0, size=batch_size)
            values = np.where(np.random.random(batch_size) < 0.8, 0, values)
        else:
            values = np.random.normal(10, 5, size=batch_size)
            values = np.maximum(values, 0)
        
        test_features[feat_name] = tf.constant(values.reshape(-1, 1), dtype=tf.float32)
    
    # ä½¿ç”¨åŠ¨æ€åˆ†æ¡¶å¤„ç†
    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())
        
        try:
            embeddings, names = create_dynamic_bucket_feats_simple(
                test_features, dense_features_1d, new_bucketing,
                log_dict={}, need_log1p=False, dim=4, all_feat_suffix="test"
            )
            
            print(f"  æˆåŠŸç”Ÿæˆ {len(embeddings)} ä¸ªembedding")
            for i, (emb, name) in enumerate(zip(embeddings, names)):
                emb_value = sess.run(emb)
                print(f"    {name}: embedding shape {emb_value.shape}, èŒƒå›´ [{emb_value.min():.4f}, {emb_value.max():.4f}]")
            
            print("âœ… åŠ¨æ€åˆ†æ¡¶åŠŸèƒ½æµ‹è¯•é€šè¿‡ï¼")
            
        except Exception as e:
            print(f"âŒ åŠ¨æ€åˆ†æ¡¶åŠŸèƒ½æµ‹è¯•å¤±è´¥: {e}")
            return False
    
    # 6. æ¸…ç†æµ‹è¯•æ–‡ä»¶
    if os.path.exists("./test_bucket_points.json"):
        os.remove("./test_bucket_points.json")
        print("7. æ¸…ç†æµ‹è¯•æ–‡ä»¶å®Œæˆ")
    
    return True

def test_comparison():
    """
    å¯¹æ¯”é™æ€åˆ†æ¡¶å’ŒåŠ¨æ€åˆ†æ¡¶çš„æ•ˆæœ
    """
    print("\n=== é™æ€åˆ†æ¡¶ vs åŠ¨æ€åˆ†æ¡¶å¯¹æ¯” ===")
    
    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    batch_size = 1000
    feature_name = 'fc_send_cnt_30m_qc_product_id_qp_roi2'
    
    # ç”Ÿæˆç¬¦åˆå®é™…åˆ†å¸ƒçš„æ•°æ®
    values = np.random.exponential(scale=5.0, size=batch_size)
    values = np.where(np.random.random(batch_size) < 0.8, 0, values)
    
    features = {feature_name: tf.constant(values.reshape(-1, 1), dtype=tf.float32)}
    
    # é™æ€åˆ†æ¡¶ç‚¹
    static_buckets = [0.001, 7.0, 11.0, 19.0, 26.0, 34.0, 42.0, 51.0, 60.0, 74.0]
    
    # åŠ¨æ€åˆ†æ¡¶ç‚¹ï¼ˆæ¨¡æ‹Ÿï¼‰
    dynamic_buckets = np.percentile(values[values > 0], [10, 20, 30, 40, 50, 60, 70, 80, 90])
    dynamic_buckets = dynamic_buckets[dynamic_buckets > 0].tolist()
    
    print(f"ç‰¹å¾: {feature_name}")
    print(f"æ•°æ®åˆ†å¸ƒ: æœ€å°å€¼={values.min():.3f}, æœ€å¤§å€¼={values.max():.3f}, å‡å€¼={values.mean():.3f}")
    print(f"é™æ€åˆ†æ¡¶ç‚¹: {static_buckets}")
    print(f"åŠ¨æ€åˆ†æ¡¶ç‚¹: {dynamic_buckets}")
    
    # è®¡ç®—åˆ†æ¡¶æ•ˆæœ
    def calculate_bucket_distribution(values, buckets):
        bucket_counts = np.zeros(len(buckets) + 1)
        for val in values:
            bucket_idx = 0
            for i, bucket in enumerate(buckets):
                if val <= bucket:
                    bucket_idx = i
                    break
            else:
                bucket_idx = len(buckets)
            bucket_counts[bucket_idx] += 1
        return bucket_counts
    
    static_dist = calculate_bucket_distribution(values, static_buckets)
    dynamic_dist = calculate_bucket_distribution(values, dynamic_buckets)
    
    print(f"\né™æ€åˆ†æ¡¶åˆ†å¸ƒ: {static_dist}")
    print(f"åŠ¨æ€åˆ†æ¡¶åˆ†å¸ƒ: {dynamic_dist}")
    
    # è®¡ç®—ç©ºæ¡¶æ•°é‡
    static_empty = np.sum(static_dist == 0)
    dynamic_empty = np.sum(dynamic_dist == 0)
    
    print(f"\né™æ€åˆ†æ¡¶ç©ºæ¡¶æ•°é‡: {static_empty}")
    print(f"åŠ¨æ€åˆ†æ¡¶ç©ºæ¡¶æ•°é‡: {dynamic_empty}")
    
    if dynamic_empty < static_empty:
        print("âœ… åŠ¨æ€åˆ†æ¡¶å‡å°‘äº†ç©ºæ¡¶æ•°é‡ï¼Œæé«˜äº†åˆ†æ¡¶æ•ˆç‡")
    else:
        print("âš ï¸ åŠ¨æ€åˆ†æ¡¶ç©ºæ¡¶æ•°é‡æœªå‡å°‘")

if __name__ == "__main__":
    # è¿è¡Œæµ‹è¯•
    success = test_dynamic_bucketing()
    
    if success:
        test_comparison()
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
    else:
        print("\nâŒ æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä»£ç ")
